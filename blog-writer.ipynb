{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-17T03:17:56.746163Z",
     "iopub.status.busy": "2024-11-17T03:17:56.745392Z",
     "iopub.status.idle": "2024-11-17T03:17:57.098386Z",
     "shell.execute_reply": "2024-11-17T03:17:57.097611Z",
     "shell.execute_reply.started": "2024-11-17T03:17:56.746116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:17:59.209174Z",
     "iopub.status.busy": "2024-11-17T03:17:59.208714Z",
     "iopub.status.idle": "2024-11-17T03:18:00.441114Z",
     "shell.execute_reply": "2024-11-17T03:18:00.440151Z",
     "shell.execute_reply.started": "2024-11-17T03:17:59.209141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def telegram_message(message, bot_token, chat_id):\n",
    "    \"\"\"\n",
    "    Send a message using the Telegram bot.\n",
    "    \n",
    "    Parameters:\n",
    "    - message (str): The message to send.\n",
    "    - bot_token (str): The token for the Telegram bot.\n",
    "    - chat_id (int): The chat ID to send the message to.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\n",
    "    data = {\"chat_id\": chat_id, \"text\": message}\n",
    "    response = requests.post(url, data=data)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Message sent: {message}\")\n",
    "    else:\n",
    "        print(\"Failed to send message:\", response.text)\n",
    "\n",
    "# Example usage\n",
    "bot_token = \"your_bot_token_here\"  # Replace with your bot token\n",
    "chat_id = 123456789  # Replace with your chat ID\n",
    "telegram_message(\"📋 Bloggy Automation started 🚀🚀🚀🚀\", bot_token, chat_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:18:13.045099Z",
     "iopub.status.busy": "2024-11-17T03:18:13.044388Z",
     "iopub.status.idle": "2024-11-17T03:18:57.887079Z",
     "shell.execute_reply": "2024-11-17T03:18:57.886053Z",
     "shell.execute_reply.started": "2024-11-17T03:18:13.045059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "! pip install feedparser requests beautifulsoup4 sentence-transformers scikit-learn google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client boto3 pillow\n",
    "!pip install diffusers transformers accelerate scipy safetensors\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:18:57.889332Z",
     "iopub.status.busy": "2024-11-17T03:18:57.888974Z",
     "iopub.status.idle": "2024-11-17T03:19:15.912287Z",
     "shell.execute_reply": "2024-11-17T03:19:15.911402Z",
     "shell.execute_reply.started": "2024-11-17T03:18:57.889283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import subprocess\n",
    "import logging\n",
    "import re\n",
    "from huggingface_hub import login\n",
    "from diffusers import StableDiffusionPipeline # Add EulerDiscreteScheduler here\n",
    "import torch\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:19:15.914336Z",
     "iopub.status.busy": "2024-11-17T03:19:15.913746Z",
     "iopub.status.idle": "2024-11-17T03:21:26.982070Z",
     "shell.execute_reply": "2024-11-17T03:21:26.981097Z",
     "shell.execute_reply.started": "2024-11-17T03:19:15.914299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Fetch and Filter Articles from Different Sources\n",
    "\n",
    "# Function to fetch and filter RSS feed data from Towards Data Science\n",
    "def fetch_rss_feed(url):\n",
    "    articles = []\n",
    "    try:\n",
    "        feed = feedparser.parse(url)\n",
    "        restricted_phrases = [\"Continue reading on Towards Data Science\", \"source=rss\"]\n",
    "        \n",
    "        for entry in feed.entries[:20]:  # Limit to latest 20 entries\n",
    "            if not any(phrase in entry.summary for phrase in restricted_phrases):\n",
    "                summary_soup = BeautifulSoup(entry.summary, \"html.parser\")\n",
    "                clean_summary = summary_soup.get_text(separator=\" \", strip=True)\n",
    "                \n",
    "                articles.append({\n",
    "                    'title': entry.title,\n",
    "                    'link': entry.link,\n",
    "                    'content': clean_summary\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching RSS feed: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Function to fetch articles from KDNuggets\n",
    "def fetch_kdnuggets_articles():\n",
    "    base_url = \"https://www.kdnuggets.com/\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            for a_tag in soup.find_all('a'):\n",
    "                b_tag = a_tag.find('b')\n",
    "                if b_tag:\n",
    "                    title = b_tag.get_text(strip=True)\n",
    "                    link = a_tag.get('href')\n",
    "                    if link and not link.startswith('http'):\n",
    "                        link = base_url + link\n",
    "                    \n",
    "                    try:\n",
    "                        page_response = requests.get(link, headers=headers)\n",
    "                        page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "                        post_div = page_soup.find('div', id='post-')\n",
    "                        content = post_div.get_text(strip=True) if post_div else \"Content not found\"\n",
    "                        \n",
    "                        articles.append({\n",
    "                            'title': title,\n",
    "                            'link': link,\n",
    "                            'content': content\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error fetching KDNuggets article at {link}: {e}\")\n",
    "                    time.sleep(1 + random.uniform(0, 1))\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching KDNuggets homepage: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Function to fetch articles from Dev.to\n",
    "def fetch_devto_articles():\n",
    "    url = \"https://dev.to/t/ai/latest\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        for article in soup.find_all(\"div\", class_=\"crayons-story\"):\n",
    "            title_tag = article.find(\"h2\", class_=\"crayons-story__title\")\n",
    "            link_tag = title_tag.find(\"a\") if title_tag else None\n",
    "            \n",
    "            if title_tag and link_tag:\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                link = f\"https://dev.to{link_tag['href']}\"\n",
    "                \n",
    "                try:\n",
    "                    article_response = requests.get(link, headers=headers)\n",
    "                    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "                    content_div = article_soup.find(\"div\", class_=\"crayons-article__body text-styles spec__body\")\n",
    "                    content = content_div.get_text(strip=True) if content_div else \"Content not found\"\n",
    "                    \n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'link': link,\n",
    "                        'content': content\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching Dev.to article at {link}: {e}\")\n",
    "                \n",
    "                time.sleep(1 + random.uniform(0, 1))\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Dev.to homepage: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Function to fetch articles from NVIDIA blog\n",
    "def fetch_nvidia_blog_articles():\n",
    "    url = \"https://developer.nvidia.com/blog/recent-posts/\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            for link_tag in soup.find_all('a', class_='carousel-row-slide__link'):\n",
    "                title_span = link_tag.find('span', class_='visually-hidden')\n",
    "                if title_span:\n",
    "                    title = title_span.get_text(strip=True)\n",
    "                    link = link_tag['href']\n",
    "                    if not link.startswith('http'):\n",
    "                        link = 'https://developer.nvidia.com' + link\n",
    "                    \n",
    "                    try:\n",
    "                        post_response = requests.get(link, headers=headers)\n",
    "                        post_soup = BeautifulSoup(post_response.content, 'html.parser')\n",
    "                        content_div = post_soup.find('div', class_='entry-content')\n",
    "                        content = content_div.get_text(separator=\"\\n\", strip=True) if content_div else \"Content not found\"\n",
    "                        \n",
    "                        articles.append({\n",
    "                            'title': title,\n",
    "                            'link': link,\n",
    "                            'content': content\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error fetching NVIDIA article at {link}: {e}\")\n",
    "                    \n",
    "                    time.sleep(1 + random.uniform(0, 1))\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching NVIDIA homepage: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Function to save articles to JSON with timestamp\n",
    "def save_articles_to_json():\n",
    "    all_articles = {\n",
    "        \"Towards Data Science\": fetch_rss_feed(\"https://towardsdatascience.com/feed\"),\n",
    "        \"KDNuggets\": fetch_kdnuggets_articles(),\n",
    "        \"Dev.to\": fetch_devto_articles(),\n",
    "        \"NVIDIA Blog\": fetch_nvidia_blog_articles()\n",
    "    }\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%d-%m-%y-%H-%M\")\n",
    "    file_name = f\"{current_time}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(file_name, \"w\") as json_file:\n",
    "            json.dump(all_articles, json_file, indent=4)\n",
    "        print(f\"Articles saved to '{file_name}'.\")\n",
    "        return file_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving articles to JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run to save and get the file path\n",
    "saved_file_path = save_articles_to_json()\n",
    "\n",
    "\n",
    "telegram_message(\"Documentation gathered successfully! 📑✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:21:26.984725Z",
     "iopub.status.busy": "2024-11-17T03:21:26.984398Z",
     "iopub.status.idle": "2024-11-17T03:21:26.993226Z",
     "shell.execute_reply": "2024-11-17T03:21:26.992305Z",
     "shell.execute_reply.started": "2024-11-17T03:21:26.984691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to load JSON data\n",
    "def load_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load data using the saved file path\n",
    "if saved_file_path:\n",
    "    data = load_json(f\"/kaggle/working/{saved_file_path}\")\n",
    "else:\n",
    "    print(\"No file path available to load data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:21:26.994907Z",
     "iopub.status.busy": "2024-11-17T03:21:26.994528Z",
     "iopub.status.idle": "2024-11-17T03:22:20.671443Z",
     "shell.execute_reply": "2024-11-17T03:22:20.670343Z",
     "shell.execute_reply.started": "2024-11-17T03:21:26.994866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "!sudo apt-get install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:22:20.673362Z",
     "iopub.status.busy": "2024-11-17T03:22:20.673005Z",
     "iopub.status.idle": "2024-11-17T03:22:56.692221Z",
     "shell.execute_reply": "2024-11-17T03:22:56.691101Z",
     "shell.execute_reply.started": "2024-11-17T03:22:20.673326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a Python function to start the Ollama API server in a separate thread with logging suppression\n",
    "def ollama():\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "    # Redirect stderr to null to suppress GIN logs\n",
    "    subprocess.Popen([\"ollama\", \"serve\"], stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Start the Ollama server in a separate thread\n",
    "ollama_thread = threading.Thread(target=ollama)\n",
    "ollama_thread.start()\n",
    "\n",
    "# Pull the model (this will suppress output as well)\n",
    "!ollama pull llama3.2:3b \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:22:56.695050Z",
     "iopub.status.busy": "2024-11-17T03:22:56.694116Z",
     "iopub.status.idle": "2024-11-17T03:22:56.706030Z",
     "shell.execute_reply": "2024-11-17T03:22:56.705231Z",
     "shell.execute_reply.started": "2024-11-17T03:22:56.695003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the llama function\n",
    "def llama(prompt_or_messages, temperature, max_tokens, raw=False, debug=False):\n",
    "    # Define the model name based on the provided model_size\n",
    "    model = 'llama3.2:3b'\n",
    "    \n",
    "    # Determine if input is a string (prompt) or list (messages for chat)\n",
    "    if isinstance(prompt_or_messages, str):\n",
    "        prompt = prompt_or_messages\n",
    "        url = \"http://127.0.0.1:11434/v1/completions\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    else:\n",
    "        messages = prompt_or_messages\n",
    "        url = \"http://127.0.0.1:11434/v1/chat/completions\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"stop\": [\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    "            \"messages\": messages\n",
    "        }\n",
    "\n",
    "    if debug:\n",
    "        print(\"Payload:\", payload)\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "        res = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Request failed: {e}\")\n",
    "\n",
    "    if 'error' in res:\n",
    "        raise Exception(f\"API Error: {res['error']}\")\n",
    "\n",
    "    if raw:\n",
    "        return res\n",
    "\n",
    "    # Return the relevant response depending on prompt or message input\n",
    "    if isinstance(prompt_or_messages, str):\n",
    "        return res['choices'][0].get('text', '')\n",
    "    else:\n",
    "        return res['choices'][0].get('message', {}).get('content', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:22:56.707777Z",
     "iopub.status.busy": "2024-11-17T03:22:56.707311Z",
     "iopub.status.idle": "2024-11-17T03:23:22.053002Z",
     "shell.execute_reply": "2024-11-17T03:23:22.051834Z",
     "shell.execute_reply.started": "2024-11-17T03:22:56.707734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Log in to Hugging Face Hub\n",
    "huggingface_token = \"your_huggingface_token_here\"  # Replace with your actual Hugging Face token\n",
    "login(token=huggingface_token)\n",
    "\n",
    "# Model and device setup\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "device = \"cuda\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "telegram_message(\"LLM model downloaded successfully! 🤖✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:23:22.054808Z",
     "iopub.status.busy": "2024-11-17T03:23:22.054480Z",
     "iopub.status.idle": "2024-11-17T03:24:49.777507Z",
     "shell.execute_reply": "2024-11-17T03:24:49.776365Z",
     "shell.execute_reply.started": "2024-11-17T03:23:22.054775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "system_message = \"Extract the main topic or keyword from the following article content and summarize it in one sentence.\"\n",
    "\n",
    "def extract_topics(data):\n",
    "    for publication, articles in data.items():  # Access each publication key and its article list\n",
    "        for article in articles:  # Iterate over each article within the publication\n",
    "            content = article.get(\"content\", \"\")\n",
    "            if content:\n",
    "                # Use the first 500 characters of content if available\n",
    "                truncated_content = content[:500]\n",
    "                \n",
    "                # OpenAI API request to extract main topic from content\n",
    "                try:\n",
    "                    prompt = (\n",
    "                                \"<|begin_of_text|>\"\n",
    "                                \"<|start_header_id|>system<|end_header_id|>\"\n",
    "                                f\"{system_message}\"\n",
    "                                \"<|eot_id|>\"    \n",
    "                                \"<|start_header_id|>user<|end_header_id|>\"\n",
    "                                f\"{truncated_content}....\"\n",
    "                                \"<|eot_id|>\"\n",
    "                                \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "                            )\n",
    "                    \n",
    "                    # Get the response text and store it in a new field in data\n",
    "                    main_topic = llama(prompt, max_tokens=4096, temperature=0.3)\n",
    "                    \n",
    "                    article[\"main_topic\"] = main_topic\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing content '{truncated_content}': {e}\")\n",
    "                    article[\"main_topic\"] = \"Error in extraction\"\n",
    "    return data\n",
    "\n",
    "# Extract topics from content and add to the data structure\n",
    "data_with_topics = extract_topics(data)\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:24:49.782408Z",
     "iopub.status.busy": "2024-11-17T03:24:49.782059Z",
     "iopub.status.idle": "2024-11-17T03:24:57.825289Z",
     "shell.execute_reply": "2024-11-17T03:24:57.824325Z",
     "shell.execute_reply.started": "2024-11-17T03:24:49.782375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the all-MiniLM-L6-v2 model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def generate_embeddings(data):\n",
    "    embeddings = []\n",
    "    topics = []\n",
    "    for publication, articles in data.items():\n",
    "        for article in articles:\n",
    "            main_topic = article.get(\"main_topic\")\n",
    "            if main_topic and \"Error\" not in main_topic:  # Ignore errors\n",
    "                # Generate the embedding for the main topic\n",
    "                embedding = model.encode(main_topic)\n",
    "                embeddings.append(embedding)\n",
    "                topics.append(main_topic)\n",
    "    \n",
    "    return embeddings, topics\n",
    "\n",
    "# Generate embeddings and store topics\n",
    "embeddings, topics = generate_embeddings(data_with_topics)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:24:57.827113Z",
     "iopub.status.busy": "2024-11-17T03:24:57.826501Z",
     "iopub.status.idle": "2024-11-17T03:24:58.199369Z",
     "shell.execute_reply": "2024-11-17T03:24:58.198312Z",
     "shell.execute_reply.started": "2024-11-17T03:24:57.827079Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Convert embeddings to a 2D array if it is 1D\n",
    "if len(np.array(embeddings).shape) == 1:\n",
    "    embeddings = np.array(embeddings).reshape(-1, 1)\n",
    "\n",
    "# Set the number of clusters\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Associate each topic with a cluster label\n",
    "clustered_topics = {i: [] for i in range(num_clusters)}\n",
    "for idx, label in enumerate(labels):\n",
    "    clustered_topics[label].append(topics[idx])\n",
    "\n",
    "# Print clustered topics for each group\n",
    "for cluster, topic_list in clustered_topics.items():\n",
    "    print(f\"Cluster {cluster}: {topic_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:24:58.204040Z",
     "iopub.status.busy": "2024-11-17T03:24:58.203098Z",
     "iopub.status.idle": "2024-11-17T03:24:58.228680Z",
     "shell.execute_reply": "2024-11-17T03:24:58.227658Z",
     "shell.execute_reply.started": "2024-11-17T03:24:58.203991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Calculate Scores for Each Topic\n",
    "import random  # Simulate scores (replace this with actual logic)\n",
    "\n",
    "def calculate_relevance_score(topic):\n",
    "    # Placeholder function to simulate scoring based on frequency, recency, etc.\n",
    "    # In practice, you would calculate based on actual data metrics.\n",
    "    return random.uniform(0, 1)  # Replace with actual scoring logic\n",
    "\n",
    "# Calculate scores for each topic and select the top topics across clusters\n",
    "topic_scores = []\n",
    "\n",
    "for cluster, topic_list in clustered_topics.items():\n",
    "    for topic in topic_list:\n",
    "        relevance_score = calculate_relevance_score(topic)\n",
    "        topic_scores.append((topic, relevance_score))\n",
    "\n",
    "# Sort by relevance score in descending order and select the top 5\n",
    "top_topics = sorted(topic_scores, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Display the top 5 topics\n",
    "print(\"Top 5 Topics:\")\n",
    "for idx, (topic, score) in enumerate(top_topics, 1):\n",
    "    print(f\"Top {idx} Topic: '{topic}' with relevance score {score:.2f}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:24:58.231984Z",
     "iopub.status.busy": "2024-11-17T03:24:58.230635Z",
     "iopub.status.idle": "2024-11-17T03:24:58.291048Z",
     "shell.execute_reply": "2024-11-17T03:24:58.289749Z",
     "shell.execute_reply.started": "2024-11-17T03:24:58.231939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to retrieve content for a given topic\n",
    "def get_content_for_topic(data, topic):\n",
    "    for publication, articles in data.items():\n",
    "        for article in articles:\n",
    "            if article.get(\"main_topic\") == topic:\n",
    "                return article.get(\"content\", \"\")\n",
    "    return \"\"\n",
    "# Generate blog sections with reference to content and structure\n",
    "def generate_blog_with_references(top_topic, data):\n",
    "    # Retrieve content associated with the top topic\n",
    "    topic_content = get_content_for_topic(data, top_topic)\n",
    "    \n",
    "    if not topic_content:\n",
    "        print(f\"No content found for topic: {top_topic}\")\n",
    "        return\n",
    "    \n",
    "    # Define the prompt for generating the blog post\n",
    "    system_message = (\n",
    "        \"You will be given a topic title and content to reference. Write a detailed and engaging blog post based on this material at your own. \"\n",
    "        \"Begin with an introduction that hooks the reader and provides an overview of the topic. Break the main content into clear, well-organized sections,also add what will bo good for it, \"\n",
    "        \"each exploring a key aspect or subtopic from the reference content. Add any relevant examples, actionable tips, or recent data to enrich the post \"\n",
    "        \"and make it practical for readers.\\n\\nWrite in a friendly, conversational tone that draws readers in, aiming for a length of about 1000-1,200 words. \"\n",
    "        \"Conclude with a summary of the key points and a call-to-action that encourages readers to engage further, such as by sharing their thoughts or exploring related resources.\"\n",
    "    )\n",
    "\n",
    "    # Format the messages for the initial blog generation\n",
    "   \n",
    "    question = f\"Write blog with this Reference Topic: {top_topic}.\\n Content: {topic_content[:650]}...\"\n",
    "    prompt = (\n",
    "        \"<|begin_of_text|>\"\n",
    "        \"<|start_header_id|>system<|end_header_id|>\"\n",
    "        f\"{system_message}\"\n",
    "        \"<|eot_id|>\"    \n",
    "        \"<|start_header_id|>user<|end_header_id|>\"\n",
    "        f\"{question}\"\n",
    "        \"<|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "    # Generate the initial blog post\n",
    "    blog_post = llama(prompt_or_messages=prompt, max_tokens=4096, temperature=0.6)\n",
    "\n",
    "    # Second LLM pass for content refinement to ensure HTML format and readability\n",
    "    refinement_prompt = (\n",
    "        f'''Refine the following blog post for better readability, coherence, and completeness. Remove redundancy, ensure all sentences are complete, and make it concise without losing important details. \n",
    "          Do not mention any of the changes made.The output format will be like this type <h1>title<h1/>.  Blog Post: \\n {blog_post}''')\n",
    "\n",
    "    refinement_messages = [{\"role\": \"user\", \"content\": refinement_prompt}]\n",
    "    refined_blog_post = llama(prompt_or_messages=refinement_messages, max_tokens=4096, temperature=0.6)\n",
    "\n",
    "    return refined_blog_post\n",
    "\n",
    "# Function to generate trending tags based on the content\n",
    "def generate_title(topic_content):\n",
    "    # Define the prompt to generate tags in a plain list format\n",
    "    title_prompt = (\n",
    "    \"Create an engaging, SEO-friendly title that captures the core message of the following content. \"\n",
    "    \"Make it attention-grabbing and informative to appeal to online readers and maximize search visibility. \"\n",
    "    \"Keep the title concise and relevant to the content's primary focus. without any additional words or formatting.\\n\\n\"\n",
    "    \"Don't write 'Here are a few options for an engaging and SEO-friendly title ...' Don't write these type, Write only the title \"\n",
    "    f\"Content: '{topic_content[:500]}...'\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Format messages for generating tags\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": title_prompt}\n",
    "        ]\n",
    "\n",
    "    # Generate the tags\n",
    "    title_response = llama(prompt_or_messages=messages, max_tokens=20, temperature=0.9)\n",
    "    \n",
    "    # Process the response to ensure it's a list of tags\n",
    "    #tags = [tag.strip() for tag in tags_response.split(\",\") if tag.strip()]\n",
    "\n",
    "    return title_response\n",
    "\n",
    "# Function to generate trending tags based on the content\n",
    "def generate_trending_tags(topic_content):\n",
    "    # Define the prompt to generate tags in a plain list format\n",
    "    tags_prompt = (\n",
    "        \"Based on the following content, generate 5 trending , common and relevant tags that capture key aspects of the topic. \"\n",
    "        \"The tags should be suitable for SEO and popular among online audiences interested in this topic. \"\n",
    "        \"Return only the tags as a comma-separated list, without any additional words or formatting.\\n\\n\"\n",
    "       \n",
    "        f\"Content: '{topic_content[:500]}...'\"\n",
    "    )\n",
    "\n",
    "    # Format messages for generating tags\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": tags_prompt}\n",
    "    ]\n",
    "\n",
    "    # Generate the tags\n",
    "    tags_response = llama(prompt_or_messages=messages, max_tokens=20, temperature=0.9)\n",
    "    \n",
    "    # Process the response to ensure it's a list of tags\n",
    "    tags = [tag.strip() for tag in tags_response.split(\",\") if tag.strip()]\n",
    "\n",
    "    return tags\n",
    "\n",
    "\n",
    "def generate_image_prompt(topic_content):\n",
    "    # Define the prompt to generate an image description\n",
    "    image_prompt_instructions = (\n",
    "        \"Using the following content, create a realistic and visually descriptive prompt for generating an image suitable for an article or blog post. \"\n",
    "        \"The prompt should describe the scene with natural, true-to-life details, focusing on accurate colors, textures, lighting, and setting. \"\n",
    "        \"Include elements that convey a professional, polished look as seen in high-quality editorial photography. \"\n",
    "        \"Avoid any abstract or overly stylized details, aiming instead for a straightforward, genuine representation. \"\n",
    "        \"Return the prompt as a single, complete sentence without additional formatting.\\n\\n\"\n",
    "        f\"Content: '{topic_content[:500]}...'\"\n",
    "    )\n",
    "    \n",
    "    # Format messages for generating the realistic image prompt\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": image_prompt_instructions}\n",
    "    ]\n",
    "    \n",
    "    # Generate the real-type image prompt\n",
    "    image_prompt_response = llama(prompt_or_messages=messages, max_tokens=60, temperature=0.8)\n",
    "    \n",
    "    # Process the response to ensure it’s a complete sentence\n",
    "    image_prompt = image_prompt_response.strip()\n",
    "    \n",
    "    return image_prompt\n",
    "\n",
    "    # Format messages for generating the image prompt\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": image_prompt_instructions}\n",
    "    ]\n",
    "    \n",
    "    # Generate the image prompt\n",
    "    image_prompt_response = llama(prompt_or_messages=messages, max_tokens=60, temperature=0.8)\n",
    "    \n",
    "    # Process the response to ensure it’s a complete sentence\n",
    "    image_prompt = image_prompt_response.strip()\n",
    "    \n",
    "    return f\"Generate a realistic image with this: {image_prompt}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:24:58.294545Z",
     "iopub.status.busy": "2024-11-17T03:24:58.293169Z",
     "iopub.status.idle": "2024-11-17T03:25:53.527630Z",
     "shell.execute_reply": "2024-11-17T03:25:53.526697Z",
     "shell.execute_reply.started": "2024-11-17T03:24:58.294501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Define your bot token and chat ID\n",
    "TOKEN = \"your_telegram_bot_token_here\"  # Replace with your actual bot token\n",
    "CHAT_ID = \"your_chat_id_here\"  # Replace with your actual chat ID\n",
    "\n",
    "# Function to send a message with inline buttons\n",
    "def send_telegram_options(top_topics):\n",
    "    url = f\"https://api.telegram.org/bot{TOKEN}/sendMessage\"\n",
    "    \n",
    "    # Define options as inline buttons\n",
    "    options = [\n",
    "        [{\"text\": \"Topic 1\", \"callback_data\": \"Topic 1\"}],\n",
    "        [{\"text\": \"Topic 2\", \"callback_data\": \"Topic 2\"}],\n",
    "        [{\"text\": \"Topic 3\", \"callback_data\": \"Topic 3\"}],\n",
    "        [{\"text\": \"Topic 4\", \"callback_data\": \"Topic 4\"}],\n",
    "        [{\"text\": \"Topic 5\", \"callback_data\": \"Topic 5\"}],\n",
    "    ]\n",
    "    \n",
    "    # Message with inline buttons\n",
    "    data = {\n",
    "        \"chat_id\": CHAT_ID,\n",
    "        \"text\": f'''Please select one of the following Topic: \\n\\n Topic 1: \\n {top_topics[0][0]}\\n\\n Topic 2: \\n{top_topics[1][0]} \\n\\n Topic 3: \\n {top_topics[2][0]} \\n\\n Topic 4: \\n {top_topics[3][0]}\\n\\n Topic 5:\\n{top_topics[4][0]} \\n\\n''',\n",
    "        \"reply_markup\": {\"inline_keyboard\": options}\n",
    "    }\n",
    "    \n",
    "    # Send the message with buttons\n",
    "    response = requests.post(url, json=data)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Options sent successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to send options:\", response.text)\n",
    "\n",
    "# Function to send a plain message (used to confirm selection)\n",
    "def send_telegram_message(chat_id, message):\n",
    "    url = f\"https://api.telegram.org/bot{TOKEN}/sendMessage\"\n",
    "    data = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": message\n",
    "    }\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.status_code == 200\n",
    "\n",
    "# Clear old updates to avoid confusion\n",
    "def clear_old_updates():\n",
    "    url = f\"https://api.telegram.org/bot{TOKEN}/getUpdates\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if the response contains updates\n",
    "    if data[\"ok\"] and \"result\" in data and len(data[\"result\"]) > 0:\n",
    "        # Mark all past updates as processed\n",
    "        last_update_id = data[\"result\"][-1][\"update_id\"]\n",
    "        requests.get(f\"{url}?offset={last_update_id + 1}\")\n",
    "    else:\n",
    "        print(\"No old updates to clear.\")\n",
    "\n",
    "# Function to get the user's selected option\n",
    "def get_user_selected_option():\n",
    "    url = f\"https://api.telegram.org/bot{TOKEN}/getUpdates\"\n",
    "    last_update_id = None\n",
    "    timeout = time.time() + 600  # 10 minutes from now\n",
    "\n",
    "    while time.time() < timeout:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        if data[\"ok\"] and \"result\" in data:\n",
    "            for update in data[\"result\"]:\n",
    "                # Skip processed updates\n",
    "                if update[\"update_id\"] == last_update_id:\n",
    "                    continue\n",
    "                last_update_id = update[\"update_id\"]\n",
    "                \n",
    "                # Check if there's a callback query (user clicked a button)\n",
    "                if \"callback_query\" in update:\n",
    "                    callback_data = update[\"callback_query\"][\"data\"]\n",
    "                    # Send confirmation back to the user\n",
    "                    send_telegram_message(CHAT_ID, f\"You selected: {callback_data}\")\n",
    "                    return callback_data\n",
    "        time.sleep(2)  # Check every 2 seconds\n",
    "\n",
    "    send_telegram_message(CHAT_ID, \"No selection was made within 10 minutes.\")\n",
    "    send_telegram_message(CHAT_ID, \"Proceed with Topic 1\")\n",
    "    return None\n",
    "\n",
    "# Placeholder for top topics\n",
    "top_topics = [\n",
    "    [\"Details for Topic 1\"],\n",
    "    [\"Details for Topic 2\"],\n",
    "    [\"Details for Topic 3\"],\n",
    "    [\"Details for Topic 4\"],\n",
    "    [\"Details for Topic 5\"],\n",
    "]\n",
    "\n",
    "# Clear old updates to ensure we're only processing new interactions\n",
    "clear_old_updates()\n",
    "\n",
    "# Send the options to the user\n",
    "send_telegram_options(top_topics)\n",
    "\n",
    "# Get the user's selected option\n",
    "selected_option = get_user_selected_option()\n",
    "\n",
    "# Process based on the selected option\n",
    "if selected_option == \"Topic 1\":\n",
    "    print(\"Processing Topic 1\")\n",
    "    top_topic = top_topics[0][0]\n",
    "elif selected_option == \"Topic 2\":\n",
    "    print(\"Processing Topic 2\")\n",
    "    top_topic = top_topics[1][0]\n",
    "elif selected_option == \"Topic 3\":\n",
    "    print(\"Processing Topic 3\")\n",
    "    top_topic = top_topics[2][0]\n",
    "elif selected_option == \"Topic 4\":\n",
    "    print(\"Processing Topic 4\")\n",
    "    top_topic = top_topics[3][0]\n",
    "elif selected_option == \"Topic 5\":\n",
    "    print(\"Processing Topic 5\")\n",
    "    top_topic = top_topics[4][0]\n",
    "else:\n",
    "    print(\"No valid option selected within the time limit.\")\n",
    "    send_telegram_message(CHAT_ID, \"No valid option selected within the time limit.\")\n",
    "    print(\"Processing Topic 1\")\n",
    "    top_topic = top_topics[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:25:53.554877Z",
     "iopub.status.busy": "2024-11-17T03:25:53.554501Z",
     "iopub.status.idle": "2024-11-17T03:26:49.040743Z",
     "shell.execute_reply": "2024-11-17T03:26:49.039527Z",
     "shell.execute_reply.started": "2024-11-17T03:25:53.554832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run the function for the top topic\n",
    "  # First item in top topics list\n",
    "final_blog_post = generate_blog_with_references(top_topic, data_with_topics)\n",
    "title=generate_title(final_blog_post)\n",
    "tag = generate_trending_tags(final_blog_post)\n",
    "image_prompt=generate_image_prompt(final_blog_post)\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:26:49.042429Z",
     "iopub.status.busy": "2024-11-17T03:26:49.042091Z",
     "iopub.status.idle": "2024-11-17T03:26:49.050388Z",
     "shell.execute_reply": "2024-11-17T03:26:49.049466Z",
     "shell.execute_reply.started": "2024-11-17T03:26:49.042395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Title: {title}\")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(final_blog_post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:28:54.245961Z",
     "iopub.status.busy": "2024-11-17T03:28:54.245060Z",
     "iopub.status.idle": "2024-11-17T03:29:18.218339Z",
     "shell.execute_reply": "2024-11-17T03:29:18.217308Z",
     "shell.execute_reply.started": "2024-11-17T03:28:54.245919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set your desired dimensions here\n",
    "width = 768  # or any width you want\n",
    "height = 512  # or any height you want\n",
    "\n",
    "# Generate the image with the specified dimensions\n",
    "\n",
    "image = pipe(prompt=image_prompt, height=height, width=width).images[0]\n",
    "\n",
    "# Save the generated image\n",
    "image.save(\"/kaggle/working/image.png\")\n",
    "\n",
    "telegram_message(\"Blog generation completed with the selected topic! 📝✨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T15:56:29.543414Z",
     "iopub.status.busy": "2024-11-15T15:56:29.542997Z",
     "iopub.status.idle": "2024-11-15T15:56:29.548771Z",
     "shell.execute_reply": "2024-11-15T15:56:29.547727Z",
     "shell.execute_reply.started": "2024-11-15T15:56:29.543372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # # # In another cell, you can now print the blog post\n",
    "# # # print(\"Refined Blog Post:\\n\", final_blog_post)\n",
    "# # from IPython.display import display, Markdown\n",
    "# # # Display as markdown\n",
    "# display(Markdown(final_blog_post))\n",
    "# # final_blog_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:29:32.569460Z",
     "iopub.status.busy": "2024-11-17T03:29:32.569019Z",
     "iopub.status.idle": "2024-11-17T03:29:32.632358Z",
     "shell.execute_reply": "2024-11-17T03:29:32.631260Z",
     "shell.execute_reply.started": "2024-11-17T03:29:32.569418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import markdown\n",
    "html_output = markdown.markdown(final_blog_post)\n",
    "print(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:29:42.545863Z",
     "iopub.status.busy": "2024-11-17T03:29:42.545145Z",
     "iopub.status.idle": "2024-11-17T03:29:44.104309Z",
     "shell.execute_reply": "2024-11-17T03:29:44.103354Z",
     "shell.execute_reply.started": "2024-11-17T03:29:42.545822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Function to upload an image to Imgur\n",
    "def upload_image_to_imgur(image_path, client_id):\n",
    "    \"\"\"\n",
    "    Uploads an image to Imgur and returns the image link.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path (str): The path to the image file.\n",
    "    - client_id (str): The Imgur Client ID.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The URL of the uploaded image.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Client-ID {client_id}\"}\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        response = requests.post(\n",
    "            \"https://api.imgur.com/3/upload\",\n",
    "            headers=headers,\n",
    "            files={\"image\": image_file}\n",
    "        )\n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the link from the response\n",
    "        link = response.json().get('data', {}).get('link', 'No link found')\n",
    "        return link\n",
    "    else:\n",
    "        print(\"Failed to upload image:\", response.text)\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "client_id = \"your_imgur_client_id_here\"  # Replace with your actual Imgur Client ID\n",
    "image_path = \"path_to_your_image.png\"  # Replace with the path to your image\n",
    "image_url = upload_image_to_imgur(image_path, client_id)\n",
    "\n",
    "if image_url:\n",
    "    print(f\"Image URL: {image_url}\")\n",
    "else:\n",
    "    print(\"Image upload failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:29:54.515969Z",
     "iopub.status.busy": "2024-11-17T03:29:54.515100Z",
     "iopub.status.idle": "2024-11-17T03:29:54.528695Z",
     "shell.execute_reply": "2024-11-17T03:29:54.527723Z",
     "shell.execute_reply.started": "2024-11-17T03:29:54.515924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Format\n",
    "# import re\n",
    "# title=str(title)\n",
    "# # Remove symbols only from the start and end\n",
    "# cleaned_title = re.sub(r'^[^\\w]+|[^\\w]+$', '', title)\n",
    "\n",
    "\n",
    "def add_banner_to_html(html_output, image_url):\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_output, 'html.parser')\n",
    "    \n",
    "    # Check for the first <h1> tag\n",
    "    h1_tag = soup.find(\"h1\")\n",
    "    \n",
    "    if h1_tag:\n",
    "        # Create the <figure> with an <img> tag after the <h1> tag\n",
    "        figure_tag = soup.new_tag(\"figure\")\n",
    "        img_tag = soup.new_tag(\"img\", src=image_url)\n",
    "        figure_tag.append(img_tag)\n",
    "        h1_tag.insert_after(figure_tag)\n",
    "    else:\n",
    "        # If no <h1> tag, find the first <strong> tag\n",
    "        strong_tag = soup.find(\"strong\")\n",
    "        if strong_tag:\n",
    "            # Convert <strong> to <h1>\n",
    "            h1_tag = soup.new_tag(\"h1\")\n",
    "            h1_tag.string = strong_tag.string  # Transfer the text content\n",
    "            strong_tag.replace_with(h1_tag)\n",
    "            \n",
    "            # Create the <figure> with an <img> tag after the new <h1> tag\n",
    "            figure_tag = soup.new_tag(\"figure\")\n",
    "            img_tag = soup.new_tag(\"img\", src=image_url)\n",
    "            figure_tag.append(img_tag)\n",
    "            h1_tag.insert_after(figure_tag)\n",
    "    \n",
    "    # Convert the soup back to a string with the modifications\n",
    "    modified_html = str(soup)\n",
    "    return modified_html\n",
    "body_html=add_banner_to_html(html_output,image_url)\n",
    "print(body_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T03:30:18.294516Z",
     "iopub.status.busy": "2024-11-17T03:30:18.294115Z",
     "iopub.status.idle": "2024-11-17T03:30:20.348339Z",
     "shell.execute_reply": "2024-11-17T03:30:20.347421Z",
     "shell.execute_reply.started": "2024-11-17T03:30:18.294479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with your Medium Integration Token\n",
    "MEDIUM_INTEGRATION_TOKEN = 'your_medium_integration_token_here'\n",
    "\n",
    "# API URLs\n",
    "MEDIUM_API_BASE_URL = 'https://api.medium.com/v1'\n",
    "USER_INFO_URL = f\"{MEDIUM_API_BASE_URL}/me\"\n",
    "CREATE_POST_URL = f\"{MEDIUM_API_BASE_URL}/users/{{user_id}}/posts\"\n",
    "\n",
    "def get_user_id():\n",
    "    \"\"\"\n",
    "    Fetches the user ID associated with the Medium Integration Token.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The user ID if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {MEDIUM_INTEGRATION_TOKEN}\"\n",
    "    }\n",
    "    response = requests.get(USER_INFO_URL, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        user_info = response.json()\n",
    "        return user_info['data']['id']\n",
    "    else:\n",
    "        print(\"Error fetching user information:\", response.text)\n",
    "        return None\n",
    "\n",
    "def create_medium_post(user_id, title, content, content_format='html', tags=None, publish_status='public'):\n",
    "    \"\"\"\n",
    "    Creates a new post on Medium.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id (str): The ID of the user creating the post.\n",
    "    - title (str): The title of the post.\n",
    "    - content (str): The content of the post in the specified format.\n",
    "    - content_format (str): Format of the content, 'html' or 'markdown'.\n",
    "    - tags (list): A list of tags for the post.\n",
    "    - publish_status (str): The publishing status ('public', 'draft', or 'unlisted').\n",
    "    \n",
    "    Returns:\n",
    "    - dict: The response JSON if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {MEDIUM_INTEGRATION_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"title\": title,\n",
    "        \"contentFormat\": content_format,\n",
    "        \"content\": content,\n",
    "        \"tags\": tags or [],\n",
    "        \"publishStatus\": publish_status\n",
    "    }\n",
    "\n",
    "    response = requests.post(CREATE_POST_URL.format(user_id=user_id), headers=headers, data=json.dumps(data))\n",
    "    \n",
    "    if response.status_code == 201:\n",
    "        print(\"Post created successfully!\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Error creating post:\", response.text)\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "user_id = get_user_id()\n",
    "if user_id:\n",
    "    # Create Medium post\n",
    "    response = create_medium_post(user_id, title=title, content=body_html, content_format=\"html\", tags=tags)\n",
    "\n",
    "    if response:\n",
    "        telegram_message(\"Blog successfully posted on Medium! 🚀📘\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2024-11-17T03:33:46.132212Z",
     "iopub.status.busy": "2024-11-17T03:33:46.131787Z",
     "iopub.status.idle": "2024-11-17T03:33:48.128414Z",
     "shell.execute_reply": "2024-11-17T03:33:48.127310Z",
     "shell.execute_reply.started": "2024-11-17T03:33:46.132175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests\n",
    "import time\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# AWS S3 configurations\n",
    "BUCKET_NAME = \"your_bucket_name_here\"  # Replace with your S3 bucket name\n",
    "TOKEN_FILE_PATH = \"refresh_token.txt\"  # Path in S3 to store the refresh token\n",
    "\n",
    "# OAuth and Blogger configurations\n",
    "CLIENT_ID = \"your_google_client_id_here\"  # Replace with your Google Client ID\n",
    "CLIENT_SECRET = \"your_google_client_secret_here\"  # Replace with your Google Client Secret\n",
    "BLOG_ID = \"your_blog_id_here\"  # Replace with your Blogger blog ID\n",
    "SCOPES = [\"https://www.googleapis.com/auth/blogger\"]\n",
    "\n",
    "# Telegram bot credentials\n",
    "TOKEN = \"your_telegram_bot_token_here\"  # Replace with your Telegram bot token\n",
    "CHAT_ID = \"your_chat_id_here\"  # Replace with your Telegram chat ID\n",
    "\n",
    "# Initialize the AWS S3 client\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=\"your_aws_access_key_id_here\",  # Replace with your AWS Access Key ID\n",
    "    aws_secret_access_key=\"your_aws_secret_access_key_here\",  # Replace with your AWS Secret Access Key\n",
    "    region_name=\"your_aws_region_here\"  # Replace with your AWS region\n",
    ")\n",
    "s3_client = session.client(\"s3\")\n",
    "\n",
    "def save_token_to_s3(token):\n",
    "    \"\"\"Saves the refresh token to S3.\"\"\"\n",
    "    s3_client.put_object(Bucket=BUCKET_NAME, Key=TOKEN_FILE_PATH, Body=token)\n",
    "    print(\"Refresh token saved to S3.\")\n",
    "\n",
    "def load_token_from_s3():\n",
    "    \"\"\"Loads the refresh token from S3.\"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=BUCKET_NAME, Key=TOKEN_FILE_PATH)\n",
    "        return response[\"Body\"].read().decode(\"utf-8\")\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(\"Token file not found in S3.\")\n",
    "        send_telegram_msg(CHAT_ID, \"Token file not found in S3.\")\n",
    "        return None\n",
    "\n",
    "# Function to send a message through Telegram\n",
    "def send_telegram_msg(chat_id, message):\n",
    "    url = f\"https://api.telegram.org/bot{TOKEN}/sendMessage\"\n",
    "    data = {\"chat_id\": chat_id, \"text\": message}\n",
    "    requests.post(url, json=data)\n",
    "\n",
    "# Function to get the latest refresh token from the user (if needed)\n",
    "def get_new_refresh_token():\n",
    "    url = f\"https://api.telegram.org/bot{TOKEN}/getUpdates\"\n",
    "    last_update_id = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    while time.time() - start_time < 600:  # Wait up to 10 minutes\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "\n",
    "        if data[\"ok\"] and \"result\" in data:\n",
    "            for update in data[\"result\"]:\n",
    "                if update[\"update_id\"] == last_update_id:\n",
    "                    continue\n",
    "                last_update_id = update[\"update_id\"]\n",
    "\n",
    "                if \"message\" in update and update[\"message\"][\"chat\"][\"id\"] == int(CHAT_ID):\n",
    "                    text = update[\"message\"][\"text\"]\n",
    "                    return text\n",
    "        time.sleep(2)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Function to get credentials\n",
    "def get_credentials():\n",
    "    global REFRESH_TOKEN\n",
    "    try:\n",
    "        creds = Credentials(\n",
    "            None,\n",
    "            refresh_token=REFRESH_TOKEN,\n",
    "            client_id=CLIENT_ID,\n",
    "            client_secret=CLIENT_SECRET,\n",
    "            token_uri=\"https://oauth2.googleapis.com/token\",\n",
    "            scopes=SCOPES\n",
    "        )\n",
    "        \n",
    "        if not creds.valid:\n",
    "            creds.refresh(Request())\n",
    "        return creds\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Token refresh failed:\", e)\n",
    "        send_telegram_msg(CHAT_ID, \"Your refresh token is invalid. Please provide a new one within 10 minutes.\")\n",
    "        \n",
    "        # Request a new refresh token via Telegram\n",
    "        new_refresh_token = get_new_refresh_token()\n",
    "        if new_refresh_token:\n",
    "            REFRESH_TOKEN = new_refresh_token\n",
    "            save_token_to_s3(new_refresh_token)\n",
    "            return get_credentials()\n",
    "        else:\n",
    "            print(\"No new token provided within the time limit.\")\n",
    "            send_telegram_msg(CHAT_ID, \"No new token provided within the time limit.\")\n",
    "            return None\n",
    "\n",
    "# Function to create a blog post\n",
    "def create_blog_post(credentials, title, content):\n",
    "    service = build(\"blogger\", \"v3\", credentials=credentials)\n",
    "    \n",
    "    post = {\n",
    "        \"title\": title,\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "    post_result = service.posts().insert(blogId=BLOG_ID, body=post).execute()\n",
    "    print(f\"Post published: {post_result['url']}\")\n",
    "    send_telegram_msg(CHAT_ID, \"Blog successfully posted on Blogger! 📝✅\")\n",
    "\n",
    "# Main function to execute\n",
    "def main():\n",
    "    credentials = get_credentials()\n",
    "    if credentials:\n",
    "        create_blog_post(credentials, title, content=body_html)\n",
    "    else:\n",
    "        print(\"Credentials could not be obtained. Exiting.\")\n",
    "        send_telegram_msg(CHAT_ID, \"Credentials could not be obtained. Exiting.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T15:57:02.513482Z",
     "iopub.status.busy": "2024-11-15T15:57:02.513138Z",
     "iopub.status.idle": "2024-11-15T15:57:03.511272Z",
     "shell.execute_reply": "2024-11-15T15:57:03.510349Z",
     "shell.execute_reply.started": "2024-11-15T15:57:02.513448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "telegram_message(\"All processes completed! 🎉 Thank you for using Bloggy! 😊\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
